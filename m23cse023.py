# -*- coding: utf-8 -*-
"""notebook2cb2d76fe6.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1MN6tgFcpwxoEXEfzU48vHQ2wff-XHeTv
"""



import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import DataLoader, Dataset
from torchvision import transforms, models
from PIL import Image
import os
import matplotlib.pyplot as plt

# Adjusted dataset class with your specified paths
class ISIC2016Dataset(Dataset):
    def __init__(self, data_dir, mask_dir, image_transform=None, mask_transform=None):
        self.data_dir = data_dir
        self.mask_dir = mask_dir
        self.image_transform = image_transform
        self.mask_transform = mask_transform
        self.images = sorted([f for f in os.listdir(data_dir) if f.endswith('.jpg') or f.endswith('.png')])

    def __len__(self):
        return len(self.images)

    def __getitem__(self, idx):
        img_name = os.path.join(self.data_dir, self.images[idx])
        mask_name = os.path.join(self.mask_dir, self.images[idx].replace('.jpg', '.png').replace('.jpeg', '.png'))
        image = Image.open(img_name).convert("RGB")
        mask = Image.open(mask_name).convert("L")

        if self.image_transform:
            image = self.image_transform(image)
        if self.mask_transform:
            mask = self.mask_transform(mask)

        return image, mask


# Data Augmentations
augmentation_transforms = transforms.Compose([
    transforms.RandomRotation(degrees=30),
    transforms.RandomHorizontalFlip(p=0.5),
    transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1),
    transforms.RandomResizedCrop(128, scale=(0.8, 1.0)),
    transforms.ToTensor(),
])

mask_transformations = transforms.Compose([
    transforms.Resize((128, 128)),
    transforms.ToTensor(),
])

# Dataloaders with your paths and augmentations
train_dataset = ISIC2016Dataset(data_dir='/content/drive/MyDrive/ISIC 2016/train',
                                mask_dir='/content/drive/MyDrive/ISIC 2016/train_masks',
                                image_transform=augmentation_transforms,
                                mask_transform=mask_transformations)

test_dataset = ISIC2016Dataset(data_dir='/content/drive/MyDrive/ISIC 2016/test',
                               mask_dir='/content/drive/MyDrive/ISIC 2016/test_masks',
                               image_transform=augmentation_transforms,
                               mask_transform=mask_transformations)

train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)
test_loader = DataLoader(test_dataset, batch_size=64, shuffle=False)

# Model with Pre-Trained MobileNetV2 as Encoder and Custom Decoder
class MobileNetV2Decoder(nn.Module):
    def __init__(self):
        super(MobileNetV2Decoder, self).__init__()
        self.encoder = models.mobilenet_v2(pretrained=True).features
        for param in self.encoder.parameters():
            param.requires_grad = False

        self.decoder = nn.Sequential(
            nn.ConvTranspose2d(1280, 512, kernel_size=3, stride=2, padding=1, output_padding=1),
            nn.ReLU(),
            nn.ConvTranspose2d(512, 256, kernel_size=3, stride=2, padding=1, output_padding=1),
            nn.ReLU(),
            nn.ConvTranspose2d(256, 128, kernel_size=3, stride=2, padding=1, output_padding=1),
            nn.ReLU(),
            nn.ConvTranspose2d(128, 64, kernel_size=3, stride=2, padding=1, output_padding=1),
            nn.ReLU(),
            nn.ConvTranspose2d(64, 32, kernel_size=3, stride=2, padding=1, output_padding=1),
            nn.ReLU(),
            nn.Conv2d(32, 1, kernel_size=1),
        )
        # Explicit Upsample layer to resize output to target size
        self.upsample = nn.Upsample(size=(128, 128), mode='bilinear', align_corners=True)

    def forward(self, x):
        x = self.encoder(x)
        x = self.decoder(x)
        x = self.upsample(x)  # Upsample to match the target size
        return x


device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
print(device)
model = MobileNetV2Decoder().to(device)
criterion = nn.BCEWithLogitsLoss()
optimizer = optim.Adam(filter(lambda p: p.requires_grad, model.parameters()), lr=0.001)

# Training and validation loops
num_epochs = 25
train_losses = []
val_losses = []
train_accuracies = []  # To store training accuracies per epoch
val_accuracies = []  # To store validation accuracies per epoch

for epoch in range(num_epochs):
    model.train()
    train_loss = 0.0
    train_acc = 0.0  # Reset training accuracy for the epoch
    train_iou, train_dice = 0.0, 0.0

    for images, masks in train_loader:
        images = images.to(device)
        masks = masks.to(device)
        optimizer.zero_grad()
        outputs = model(images)
        loss = criterion(outputs, masks)
        loss.backward()
        optimizer.step()
        train_loss += loss.item() * images.size(0)

        # Calculate metrics
        output_thresholded = torch.sigmoid(outputs) > 0.5
        target_thresholded = masks > 0.5
        intersection = (output_thresholded & target_thresholded).float().sum((1, 2))
        union = (output_thresholded | target_thresholded).float().sum((1, 2))
        iou = (intersection + 1e-6) / (union + 1e-6)
        dice = (2. * intersection + 1e-6) / (output_thresholded.float().sum((1, 2)) + target_thresholded.float().sum((1, 2)) + 1e-6)
        accuracy = (output_thresholded == target_thresholded).float().sum() / target_thresholded.numel()

        train_iou += iou.mean().item() * images.size(0)
        train_dice += dice.mean().item() * images.size(0)
        train_acc += accuracy.item() * images.size(0)

    train_iou /= len(train_loader.dataset)
    train_dice /= len(train_loader.dataset)
    train_loss /= len(train_loader.dataset)
    train_losses.append(train_loss)
    train_accuracies.append(train_acc / len(train_loader.dataset))

    # Validation loop
    model.eval()
    val_loss = 0.0
    val_acc = 0.0
    val_iou, val_dice = 0.0, 0.0

    for images, masks in test_loader:
        images, masks = images.to(device), masks.to(device)
        with torch.no_grad():
            outputs = model(images)
            loss = criterion(outputs, masks)
            val_loss += loss.item() * images.size(0)

            # Calculate metrics
            output_thresholded = torch.sigmoid(outputs) > 0.5
            target_thresholded = masks > 0.5
            intersection = (output_thresholded & target_thresholded).float().sum((1, 2))
            union = (output_thresholded | target_thresholded).float().sum((1, 2))
            iou = (intersection + 1e-6) / (union + 1e-6)
            dice = (2. * intersection + 1e-6) / (output_thresholded.float().sum((1, 2)) + target_thresholded.float().sum((1, 2)) + 1e-6)
            accuracy = (output_thresholded == target_thresholded).float().sum() / target_thresholded.numel()

            val_iou += iou.mean().item() * images.size(0)
            val_dice += dice.mean().item() * images.size(0)
            val_acc += accuracy.item() * images.size(0)

    val_loss /= len(test_loader.dataset)
    val_iou /= len(test_loader.dataset)
    val_dice /= len(test_loader.dataset)

    val_losses.append(val_loss)
    val_accuracies.append(val_acc / len(test_loader.dataset))

    print(f'Epoch {epoch+1}/{num_epochs}, Train Loss: {train_loss:.4f}, Train Acc: {train_accuracies[-1]:.4f}, Val Loss: {val_loss:.4f}, Val Acc: {val_acc/len(test_loader.dataset):.4f}, Train IoU: {train_iou:.4f}, Val IoU: {val_iou:.4f}, Train Dice: {train_dice:.4f}, Val Dice: {val_dice:.4f}')

# Plotting Losses
plt.figure(figsize=(10, 4))
plt.subplot(1, 2, 1)
plt.plot(train_losses, label='Training Loss')
plt.plot(val_losses, label='Validation Loss')
plt.title('Loss during training')
plt.xlabel('Epoch')
plt.ylabel('Loss')
plt.legend()
plt.show()

plt.figure(figsize=(12, 5))
plt.subplot(1, 2, 1)
plt.plot(train_losses, label='Train Loss')
plt.plot(val_losses, label='Validation Loss')
plt.legend()
plt.title('Loss')

plt.subplot(1, 2, 2)
plt.plot(train_accuracies, label='Train Accuracy')
plt.plot(val_accuracies, label='Validation Accuracy')
plt.legend()
plt.title('Accuracy')

plt.show()

print('Finished Training')

def visualize_prediction(data_loader, model, device, num_images=5):
    model.eval()
    images, masks = next(iter(data_loader))
    images, masks = images.to(device), masks.to(device)

    with torch.no_grad():
        outputs = model(images)
        outputs = torch.sigmoid(outputs) > 0.5

    images, masks, outputs = images.cpu(), masks.cpu(), outputs.cpu()

    plt.figure(figsize=(15, 5))

    for i in range(num_images):
        plt.subplot(3, num_images, i+1)
        plt.imshow(images[i].permute(1, 2, 0))
        plt.title('Input Image')
        plt.axis('off')

    for i in range(num_images):
        plt.subplot(3, num_images, num_images+i+1)
        plt.imshow(masks[i].squeeze(), cmap='gray')
        plt.title('Ground Truth Mask')
        plt.axis('off')

    for i in range(num_images):
        plt.subplot(3, num_images, 2*num_images+i+1)
        plt.imshow(outputs[i].squeeze(), cmap='gray')
        plt.title('Predicted Mask')
        plt.axis('off')

    plt.show()

# Visualize some samples with their masks
visualize_prediction(test_loader, model, device, num_images=6)


import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import DataLoader
from pathlib import Path
import torchaudio
from torchvision.transforms import Compose
import torchaudio.transforms as T
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score, confusion_matrix, f1_score, roc_auc_score, roc_curve, auc
import pandas as pd
from sklearn.preprocessing import LabelEncoder
import wandb
import numpy as np
import matplotlib.pyplot as plt
from sklearn.metrics import roc_curve, auc
from sklearn.preprocessing import label_binarize
import itertools



class MultiHeadSelfAttention(nn.Module):
    def __init__(self, embed_dim, num_heads=1):
        super().__init__()
        self.embed_dim = embed_dim
        self.num_heads = num_heads
        self.scaling = self.embed_dim ** -0.5

        self.query = nn.Linear(embed_dim, embed_dim)
        self.key = nn.Linear(embed_dim, embed_dim)
        self.value = nn.Linear(embed_dim, embed_dim)

        self.out_proj = nn.Linear(embed_dim, embed_dim)

    def transpose_for_scores(self, x):
        new_x_shape = x.size()[:-1] + (self.num_heads, self.embed_dim // self.num_heads)
        x = x.view(*new_x_shape)
        return x.permute(0, 2, 1, 3)

    def forward(self, x):
        q = self.query(x)
        k = self.key(x)
        v = self.value(x)

        q = self.transpose_for_scores(q)
        k = self.transpose_for_scores(k)
        v = self.transpose_for_scores(v)

        attn_weights = torch.matmul(q, k.transpose(-1, -2))
        attn_weights *= self.scaling
        attn_weights = torch.nn.functional.softmax(attn_weights, dim=-1)

        attn_output = torch.matmul(attn_weights, v)
        attn_output = attn_output.transpose(1, 2).reshape(attn_output.size(0), -1, self.embed_dim)
        attn_output = self.out_proj(attn_output)

        return attn_output

class TransformerBlock(nn.Module):
    def __init__(self, embed_dim, num_heads, dropout=0.1):
        super().__init__()
        self.attention = MultiHeadSelfAttention(embed_dim, num_heads)
        self.norm1 = nn.LayerNorm(embed_dim)
        self.norm2 = nn.LayerNorm(embed_dim)

        self.ffn = nn.Sequential(
            nn.Linear(embed_dim, embed_dim),
            nn.ReLU(),
            nn.Dropout(dropout),
            nn.Linear(embed_dim, embed_dim),
        )

        self.dropout = nn.Dropout(dropout)

    def forward(self, x):
        attn_output = self.attention(x)
        x = x + self.dropout(attn_output)
        x = self.norm1(x)

        ffn_output = self.ffn(x)
        x = x + self.dropout(ffn_output)
        x = self.norm2(x)

        return x

class CustomConvNet(nn.Module):
    def __init__(self, num_classes, num_channels=40, transformer_heads=[1, 2, 4]):
        super(CustomConvNet, self).__init__()

        self.conv = nn.Sequential(
            nn.Conv1d(num_channels, 32, kernel_size=3, stride=1, padding=1),
            nn.BatchNorm1d(32),
            nn.ReLU(),
            nn.Dropout(0.5),
            nn.MaxPool1d(kernel_size=2, stride=2),

            nn.Conv1d(32, 64, kernel_size=3, stride=1, padding=1),
            nn.BatchNorm1d(64),
            nn.ReLU(),
            nn.Dropout(0.5),
            nn.MaxPool1d(kernel_size=2, stride=2),

            nn.Conv1d(64, 128, kernel_size=3, stride=1, padding=1),
            nn.BatchNorm1d(128),
            nn.ReLU(),
            nn.Dropout(0.5),
            nn.MaxPool1d(kernel_size=2, stride=2),
        )

        transformer_blocks = []
        for num_heads in transformer_heads:
            transformer_blocks.append(TransformerBlock(128, num_heads))

        self.transformer = nn.Sequential(*transformer_blocks)

        self.cls_token = nn.Parameter(torch.randn(1, 1, 128))

        self.classifier = nn.Sequential(
            nn.Linear(128, num_classes),
            nn.Softmax(dim=1)
        )

    def forward(self, x):
        x = self.conv(x)
        x = x.permute(0, 2, 1)
        cls_token = self.cls_token.expand(x.size(0), -1, -1)
        x = torch.cat([cls_token, x], dim=1)
        x = self.transformer(x)
        x = x.mean(dim=1)
        x = self.classifier(x)
        return x

def preprocess_audio(batch, sample_rate=44100):
    waveforms = []
    targets = []

    for audio_file, target in batch:
        waveform, _ = torchaudio.load(audio_file, normalize=True)

        transform = Compose([
            T.Resample(orig_freq=sample_rate, new_freq=16000),
            T.MFCC(),
            T.TimeMasking(time_mask_param=20),
            T.FrequencyMasking(freq_mask_param=30)
        ])

        processed_waveform = transform(waveform)
        waveforms.append(processed_waveform)
        targets.append(target)

    return torch.stack(waveforms).squeeze(1), torch.tensor(targets)

def train_model(model, train_loader, val_loader, criterion, optimizer, num_epochs, device, wandb_log=True):
    best_val_loss = float('inf')
    patience = 5
    epochs_no_improve = 0

    if wandb_log:
        wandb.watch(model, log='all')

    for epoch in range(num_epochs):
        model.train()
        running_loss = 0.0
        correct_predictions = 0
        total_samples = 0

        for inputs, targets in train_loader:
            inputs, targets = inputs.to(device), targets.to(device)
            optimizer.zero_grad()
            outputs = model(inputs)
            loss = criterion(outputs, targets)
            loss.backward()
            optimizer.step()
            running_loss += loss.item()

            _, predicted = torch.max(outputs, 1)
            correct_predictions += (predicted == targets).sum().item()
            total_samples += targets.size(0)

        train_loss = running_loss / len(train_loader)
        train_accuracy = correct_predictions / total_samples

        # Validation
        val_loss, val_accuracy = evaluate_model(model, val_loader, criterion, device)

        if wandb_log:
            wandb.log({'epoch': epoch + 1, 'train_loss': train_loss, 'train_accuracy': train_accuracy,
                       'val_loss': val_loss, 'val_accuracy': val_accuracy})

        print(f"Epoch [{epoch+1}/{num_epochs}], Train Loss: {train_loss:.4f}, Train Accuracy: {train_accuracy:.4f}, "
              f"Val Loss: {val_loss:.4f}, Val Accuracy: {val_accuracy:.4f}")

    print("Training completed.")

def evaluate_model(model, test_loader, criterion, device):
    model.eval()
    running_loss = 0.0
    correct_predictions = 0
    total_samples = 0

    with torch.no_grad():
        for inputs, targets in test_loader:
            inputs, targets = inputs.to(device), targets.to(device)
            outputs = model(inputs)
            loss = criterion(outputs, targets)
            running_loss += loss.item()

            _, predicted = torch.max(outputs, 1)
            correct_predictions += (predicted == targets).sum().item()
            total_samples += targets.size(0)

    loss = running_loss / len(test_loader)
    accuracy = correct_predictions / total_samples

    return loss, accuracy

def count_parameters(model):
    total_params = sum(p.numel() for p in model.parameters())
    trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)
    non_trainable_params = total_params - trainable_params
    return total_params, trainable_params, non_trainable_params

# Function to plot accuracy and loss per epoch
def plot_metrics(epochs, train_loss, val_loss, train_accuracy, val_accuracy):
    plt.figure(figsize=(10, 5))
    plt.plot(epochs, train_loss, label='Train Loss', color='blue')
    plt.plot(epochs, val_loss, label='Validation Loss', color='orange')
    plt.xlabel('Epochs')
    plt.ylabel('Loss')
    plt.title('Training and Validation Loss')
    plt.legend()
    plt.show()

    plt.figure(figsize=(10, 5))
    plt.plot(epochs, train_accuracy, label='Train Accuracy', color='blue')
    plt.plot(epochs, val_accuracy, label='Validation Accuracy', color='orange')
    plt.xlabel('Epochs')
    plt.ylabel('Accuracy')
    plt.title('Training and Validation Accuracy')
    plt.legend()
    plt.show()


def evaluate_metrics(model, test_loader, device):
    model.eval()
    all_predictions = []
    all_targets = []
    all_probs = []

    with torch.no_grad():
        for inputs, targets in test_loader:
            inputs, targets = inputs.to(device), targets.to(device)
            outputs = model(inputs)
            _, predicted = torch.max(outputs, 1)
            all_predictions.extend(predicted.cpu().numpy())
            all_targets.extend(targets.cpu().numpy())

            probs = torch.nn.functional.softmax(outputs, dim=1)
            all_probs.extend(probs.cpu().numpy())

    accuracy = accuracy_score(all_targets, all_predictions)
    confusion_mat = confusion_matrix(all_targets, all_predictions)
    f1 = f1_score(all_targets, all_predictions, average='macro')

    roc_auc = roc_auc_score(all_targets, np.array(all_probs), multi_class='ovr')

    return accuracy, confusion_mat, f1, roc_auc


def plot_roc_curve(y_true, y_score, num_classes):
    plt.figure(figsize=(10, 8))
    fpr = dict()
    tpr = dict()
    roc_auc = dict()
    for i in range(num_classes):
        fpr[i], tpr[i], _ = roc_curve(y_true[:, i], y_score[:, i])
        roc_auc[i] = auc(fpr[i], tpr[i])
        plt.plot(fpr[i], tpr[i], lw=2, label='Class %d (AUC = %0.2f)' % (i, roc_auc[i]))

    plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')
    plt.xlim([0.0, 1.0])
    plt.ylim([0.0, 1.05])
    plt.xlabel('False Positive Rate')
    plt.ylabel('True Positive Rate')
    plt.title('ROC Curve')
    plt.legend(loc="lower right")
    plt.show()


def plot_confusion_matrix(cm, classes, title='Confusion Matrix', cmap=plt.cm.Blues):
    plt.figure(figsize=(10, 8))
    plt.imshow(cm, interpolation='nearest', cmap=cmap)
    plt.title(title)
    plt.colorbar()
    tick_marks = np.arange(len(classes))
    plt.xticks(tick_marks, classes, rotation=45)
    plt.yticks(tick_marks, classes)

    thresh = cm.max() / 2.
    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):
        plt.text(j, i, format(cm[i, j], 'd'), horizontalalignment="center", color="white" if cm[i, j] > thresh else "black")

    plt.tight_layout()
    plt.ylabel('True label')
    plt.xlabel('Predicted label')

def main():
    path = '/content/drive/MyDrive/Archive/audio'
    df = pd.read_csv('/content/drive/MyDrive/Archive/meta/esc50.csv')

    esc_10_flag = True
    if esc_10_flag:
        df = df[df['esc10'] == True]

    wandb.init(project='finalll', name='missclassyfyed', config={'epochs': 100, 'learning_rate': 0.001})

    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

    label_encoder = LabelEncoder()
    df['target'] = label_encoder.fit_transform(df['category'])
    num_classes = len(label_encoder.classes_)


    train_df, test_df = train_test_split(df, test_size=0.2, random_state=42)
    train_df, val_df = train_test_split(train_df, test_size=0.1, random_state=42)

    train_loader = DataLoader([(Path(path) / filename, target) for filename, target in zip(train_df['filename'], train_df['target'])], batch_size=32, shuffle=True, collate_fn=preprocess_audio)
    val_loader = DataLoader([(Path(path) / filename, target) for filename, target in zip(val_df['filename'], val_df['target'])], batch_size=32, shuffle=False, collate_fn=preprocess_audio)
    test_loader = DataLoader([(Path(path) / filename, target) for filename, target in zip(test_df['filename'], test_df['target'])], batch_size=32, shuffle=False, collate_fn=preprocess_audio)

    model = CustomConvNet(num_classes=num_classes, num_channels=40)
    model.to(device)

    criterion = nn.CrossEntropyLoss()
    optimizer = optim.Adam(model.parameters(), lr=0.001)

    train_model(model, train_loader, val_loader, criterion, optimizer, num_epochs=100, device=device)

    test_loss, test_accuracy = evaluate_model(model, test_loader, criterion, device)
    print("Test Loss:", test_loss)
    print("Test Accuracy:", test_accuracy)

    total_params, trainable_params, non_trainable_params = count_parameters(model)
    print(f"Total Trainable Parameters: {trainable_params}")
    print(f"Total Non-trainable Parameters: {non_trainable_params}")

    test_accuracy, confusion_mat, f1, roc_auc = evaluate_metrics(model, test_loader, device)
    print("Test Accuracy:", test_accuracy)
    print("Confusion Matrix:\n", confusion_mat)
    print("F1 Score:", f1)
    print("AUC-ROC Score:", roc_auc)


    all_targets = []
    all_probs = []
    with torch.no_grad():
        for inputs, targets in test_loader:
            inputs, targets = inputs.to(device), targets.to(device)
            outputs = model(inputs)

            probs = torch.nn.functional.softmax(outputs, dim=1)
            all_probs.extend(probs.cpu().numpy())
            all_targets.extend(targets.cpu().numpy())

    all_targets = np.array(all_targets)
    all_probs = np.array(all_probs)


    all_targets_one_hot = label_binarize(all_targets, classes=np.arange(num_classes))


    plot_roc_curve(all_targets_one_hot, all_probs, num_classes)


    plot_confusion_matrix(confusion_mat, classes=label_encoder.classes_)

    wandb.finish()

if __name__ == "__main__":
    main()

    
import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import DataLoader, Dataset
from torchvision import transforms, models
from PIL import Image
import os
import matplotlib.pyplot as plt


class ISIC2016Dataset(Dataset):
    def __init__(self, data_dir, mask_dir, image_transform=None, mask_transform=None):
        self.data_dir = data_dir
        self.mask_dir = mask_dir
        self.image_transform = image_transform
        self.mask_transform = mask_transform
        self.images = sorted([f for f in os.listdir(data_dir) if f.endswith('.jpg') or f.endswith('.png')])

    def __len__(self):
        return len(self.images)

    def __getitem__(self, idx):
        img_name = os.path.join(self.data_dir, self.images[idx])
        mask_name = os.path.join(self.mask_dir, self.images[idx].replace('.jpg', '.png').replace('.jpeg', '.png'))
        image = Image.open(img_name).convert("RGB")
        mask = Image.open(mask_name).convert("L")

        if self.image_transform:
            image = self.image_transform(image)
        if self.mask_transform:
            mask = self.mask_transform(mask)

        return image, mask



from torchvision.transforms import Normalize


augmentation_transforms = transforms.Compose([
    transforms.RandomRotation(degrees=30),
    transforms.RandomHorizontalFlip(p=0.5),
    transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1),
    transforms.RandomResizedCrop(128, scale=(0.8, 1.0)),
    transforms.ToTensor(),

    Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),
])

mask_transformations = transforms.Compose([
    transforms.Resize((128, 128)),
    transforms.ToTensor(),
])


train_dataset = ISIC2016Dataset(data_dir='/kaggle/input/dataset/ISIC 2016/train',
                                mask_dir='/kaggle/input/dataset/ISIC 2016/train_masks',
                                image_transform=augmentation_transforms,
                                mask_transform=mask_transformations)

test_dataset = ISIC2016Dataset(data_dir='/kaggle/input/dataset/ISIC 2016/test',
                               mask_dir='/kaggle/input/dataset/ISIC 2016/test_masks',
                               image_transform=augmentation_transforms,
                               mask_transform=mask_transformations)

train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)
test_loader = DataLoader(test_dataset, batch_size=64, shuffle=False)


class MobileNetV2Decoder(nn.Module):
    def __init__(self):
        super(MobileNetV2Decoder, self).__init__()
        self.encoder = models.mobilenet_v2(pretrained=True).features
        for param in self.encoder.parameters():
            param.requires_grad = False

        self.decoder = nn.Sequential(
            nn.ConvTranspose2d(1280, 512, kernel_size=3, stride=2, padding=1, output_padding=1),
            nn.ReLU(),
            nn.ConvTranspose2d(512, 256, kernel_size=3, stride=2, padding=1, output_padding=1),
            nn.ReLU(),
            nn.ConvTranspose2d(256, 128, kernel_size=3, stride=2, padding=1, output_padding=1),
            nn.ReLU(),
            nn.ConvTranspose2d(128, 64, kernel_size=3, stride=2, padding=1, output_padding=1),
            nn.ReLU(),
            nn.ConvTranspose2d(64, 32, kernel_size=3, stride=2, padding=1, output_padding=1),
            nn.ReLU(),
            nn.Conv2d(32, 1, kernel_size=1),
        )

        self.upsample = nn.Upsample(size=(128, 128), mode='bilinear', align_corners=True)

    def forward(self, x):
        x = self.encoder(x)
        x = self.decoder(x)
        x = self.upsample(x)
        return x


device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
print(device)
model = MobileNetV2Decoder().to(device)
criterion = nn.BCEWithLogitsLoss()
optimizer = optim.Adam(filter(lambda p: p.requires_grad, model.parameters()), lr=0.001)


num_epochs = 25
train_losses = []
val_losses = []
train_accuracies = []
val_accuracies = []
train_ious = []
val_ious = []
train_dices = []
val_dices = []

for epoch in range(num_epochs):
    model.train()
    train_loss = 0.0
    train_acc = 0.0
    train_iou, train_dice = 0.0, 0.0

    for images, masks in train_loader:
        images = images.to(device)
        masks = masks.to(device)
        optimizer.zero_grad()
        outputs = model(images)
        loss = criterion(outputs, masks)
        loss.backward()
        optimizer.step()
        train_loss += loss.item() * images.size(0)


        output_thresholded = torch.sigmoid(outputs) > 0.5
        target_thresholded = masks > 0.5
        intersection = (output_thresholded & target_thresholded).float().sum((1, 2))
        union = (output_thresholded | target_thresholded).float().sum((1, 2))
        iou = (intersection + 1e-6) / (union + 1e-6)
        dice = (2. * intersection + 1e-6) / (output_thresholded.float().sum((1, 2)) + target_thresholded.float().sum((1, 2)) + 1e-6)
        accuracy = (output_thresholded == target_thresholded).float().sum() / target_thresholded.numel()

        train_iou += iou.mean().item() * images.size(0)
        train_dice += dice.mean().item() * images.size(0)
        train_acc += accuracy.item() * images.size(0)

    train_iou /= len(train_loader.dataset)
    train_dice /= len(train_loader.dataset)
    train_loss /= len(train_loader.dataset)
    train_losses.append(train_loss)
    train_accuracies.append(train_acc / len(train_loader.dataset))
    train_ious.append(train_iou)
    train_dices.append(train_dice)


    model.eval()
    val_loss = 0.0
    val_acc = 0.0
    val_iou, val_dice = 0.0, 0.0

    for images, masks in test_loader:
        images, masks = images.to(device), masks.to(device)
        with torch.no_grad():
            outputs = model(images)
            loss = criterion(outputs, masks)
            val_loss += loss.item() * images.size(0)


            output_thresholded = torch.sigmoid(outputs) > 0.5
            target_thresholded = masks > 0.5
            intersection = (output_thresholded & target_thresholded).float().sum((1, 2))
            union = (output_thresholded | target_thresholded).float().sum((1, 2))
            iou = (intersection + 1e-6) / (union + 1e-6)
            dice = (2. * intersection + 1e-6) / (output_thresholded.float().sum((1, 2)) + target_thresholded.float().sum((1, 2)) + 1e-6)
            accuracy = (output_thresholded == target_thresholded).float().sum() / target_thresholded.numel()

            val_iou += iou.mean().item() * images.size(0)
            val_dice += dice.mean().item() * images.size(0)
            val_acc += accuracy.item() * images.size(0)

    val_loss /= len(test_loader.dataset)
    val_iou /= len(test_loader.dataset)
    val_dice /= len(test_loader.dataset)

    val_losses.append(val_loss)
    val_accuracies.append(val_acc / len(test_loader.dataset))
    val_ious.append(val_iou)
    val_dices.append(val_dice)

    print(f'Epoch {epoch+1}/{num_epochs}, Train Loss: {train_loss:.4f}, Train Acc: {train_accuracies[-1]:.4f}, Val Loss: {val_loss:.4f}, Val Acc: {val_acc/len(test_loader.dataset):.4f}, Train IoU: {train_iou:.4f}, Val IoU: {val_iou:.4f}, Train Dice: {train_dice:.4f}, Val Dice: {val_dice:.4f}')


plt.figure(figsize=(12, 8))


salmon_pink = "#FA8072"


plt.subplot(2, 2, 1)
plt.plot(train_losses, label='Train Loss', color=salmon_pink)
plt.plot(val_losses, label='Validation Loss', color='deeppink')
plt.legend()
plt.title('Loss')


plt.subplot(2, 2, 2)
plt.plot(train_accuracies, label='Train Accuracy', color=salmon_pink)
plt.plot(val_accuracies, label='Validation Accuracy', color='deeppink')
plt.legend()
plt.title('Accuracy')


plt.subplot(2, 2, 3)
plt.plot(train_ious, label='Train IoU', color=salmon_pink)
plt.plot(val_ious, label='Validation IoU', color='deeppink')
plt.legend()
plt.title('IoU')

plt.subplot(2, 2, 4)
plt.plot(train_dices, label='Train Dice', color=salmon_pink)
plt.plot(val_dices, label='Validation Dice', color='deeppink')
plt.legend()
plt.title('Dice Score')

plt.tight_layout()
plt.show()

print('Hello Finished Training')

def visualize_prediction(data_loader, model, device, num_images=5):
    model.eval()
    images, masks = next(iter(data_loader))
    images, masks = images.to(device), masks.to(device)

    with torch.no_grad():
        outputs = model(images)
        outputs = torch.sigmoid(outputs) > 0.5

    images, masks, outputs = images.cpu(), masks.cpu(), outputs.cpu()

    plt.figure(figsize=(15, 5))

    for i in range(num_images):
        plt.subplot(3, num_images, i+1)
        plt.imshow(images[i].permute(1, 2, 0))
        plt.title('Input Image')
        plt.axis('off')

    for i in range(num_images):
        plt.subplot(3, num_images, num_images+i+1)
        plt.imshow(masks[i].squeeze(), cmap='gray')
        plt.title('Ground Truth Mask')
        plt.axis('off')

    for i in range(num_images):
        plt.subplot(3, num_images, 2*num_images+i+1)
        plt.imshow(outputs[i].squeeze(), cmap='gray')
        plt.title('Predicted Mask')
        plt.axis('off')

    plt.show()

# Visualize some samples with their masks
visualize_prediction(test_loader, model, device, num_images=6)

